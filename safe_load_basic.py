#!/usr/bin/env python3

# Copyright (c) 2022 RatWithAShotgun
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import argparse
import re
import string
import zipfile

import torch

# Loads torch saved stablediffusion weights safely and re-saves them.
# Doesn't use pickle module, just hackickly reads required info from pickle data directly.
# It's horrid but it works, only reads the same dict keys as are in normal SD 1.4 weights.

# dependencies: torch
# usage: python safe_load_basic.py [--prune] [--overwrite] path_to_input.ckpt  path_where_to_save.ckpt


SD_KEYS = {
    'alphas_cumprod':                                                                      (4000, torch.float32, torch.Size([1000])),
    'alphas_cumprod_prev':                                                                 (4000, torch.float32, torch.Size([1000])),
    'betas':                                                                               (4000, torch.float32, torch.Size([1000])),
    'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight':        (236544, torch.float32, torch.Size([77, 768])),
    'cond_stage_model.transformer.text_model.embeddings.position_ids':                     (616, torch.int64, torch.Size([1, 77])),
    'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight':           (151781376, torch.float32, torch.Size([49408, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias':          (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight':        (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias':          (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight':        (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias':              (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight':            (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias':              (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight':            (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias':   (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight': (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias':          (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight':        (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias':          (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight':        (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias':              (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight':            (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias':              (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight':            (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias':   (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight': (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias':     (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight':   (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias':           (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight':         (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias':               (12288, torch.float32, torch.Size([3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight':             (9437184, torch.float32, torch.Size([3072, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias':               (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight':             (9437184, torch.float32, torch.Size([768, 3072])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias':    (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight':  (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias':      (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight':    (2359296, torch.float32, torch.Size([768, 768])),
    'cond_stage_model.transformer.text_model.final_layer_norm.bias':                       (3072, torch.float32, torch.Size([768])),
    'cond_stage_model.transformer.text_model.final_layer_norm.weight':                     (3072, torch.float32, torch.Size([768])),
    'first_stage_model.decoder.conv_in.bias':                                              (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.conv_in.weight':                                            (73728, torch.float32, torch.Size([512, 4, 3, 3])),
    'first_stage_model.decoder.conv_out.bias':                                             (12, torch.float32, torch.Size([3])),
    'first_stage_model.decoder.conv_out.weight':                                           (13824, torch.float32, torch.Size([3, 128, 3, 3])),
    'first_stage_model.decoder.mid.attn_1.k.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.k.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.decoder.mid.attn_1.norm.bias':                                      (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.norm.weight':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.proj_out.bias':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.proj_out.weight':                                (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.decoder.mid.attn_1.q.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.q.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.decoder.mid.attn_1.v.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.attn_1.v.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.decoder.mid.block_1.conv1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_1.conv1.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.mid.block_1.conv2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_1.conv2.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.mid.block_1.norm1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_1.norm1.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_1.norm2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_1.norm2.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.conv1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.conv1.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.mid.block_2.conv2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.conv2.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.mid.block_2.norm1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.norm1.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.norm2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.mid.block_2.norm2.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.norm_out.bias':                                             (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.norm_out.weight':                                           (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.0.conv1.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.0.conv1.weight':                                 (1179648, torch.float32, torch.Size([128, 256, 3, 3])),
    'first_stage_model.decoder.up.0.block.0.conv2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.0.conv2.weight':                                 (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias':                            (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight':                          (131072, torch.float32, torch.Size([128, 256, 1, 1])),
    'first_stage_model.decoder.up.0.block.0.norm1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.0.block.0.norm1.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.0.block.0.norm2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.0.norm2.weight':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.conv1.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.conv1.weight':                                 (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.decoder.up.0.block.1.conv2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.conv2.weight':                                 (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.decoder.up.0.block.1.norm1.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.norm1.weight':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.norm2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.1.norm2.weight':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.conv1.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.conv1.weight':                                 (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.decoder.up.0.block.2.conv2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.conv2.weight':                                 (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.decoder.up.0.block.2.norm1.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.norm1.weight':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.norm2.bias':                                   (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.0.block.2.norm2.weight':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.decoder.up.1.block.0.conv1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.0.conv1.weight':                                 (4718592, torch.float32, torch.Size([256, 512, 3, 3])),
    'first_stage_model.decoder.up.1.block.0.conv2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.0.conv2.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias':                            (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight':                          (524288, torch.float32, torch.Size([256, 512, 1, 1])),
    'first_stage_model.decoder.up.1.block.0.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.1.block.0.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.1.block.0.norm2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.0.norm2.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.conv1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.conv1.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.1.block.1.conv2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.conv2.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.1.block.1.norm1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.norm1.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.norm2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.1.norm2.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.conv1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.conv1.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.1.block.2.conv2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.conv2.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.1.block.2.norm1.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.norm1.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.norm2.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.block.2.norm2.weight':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.upsample.conv.bias':                                   (1024, torch.float32, torch.Size([256])),
    'first_stage_model.decoder.up.1.upsample.conv.weight':                                 (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.decoder.up.2.block.0.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.0.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.0.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.0.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.0.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.0.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.0.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.0.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.1.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.1.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.1.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.2.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.2.block.2.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.block.2.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.upsample.conv.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.2.upsample.conv.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.0.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.0.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.0.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.0.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.0.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.0.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.0.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.0.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.1.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.1.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.1.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.conv1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.conv1.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.2.conv2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.conv2.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.decoder.up.3.block.2.norm1.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.norm1.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.norm2.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.block.2.norm2.weight':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.upsample.conv.bias':                                   (2048, torch.float32, torch.Size([512])),
    'first_stage_model.decoder.up.3.upsample.conv.weight':                                 (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.conv_in.bias':                                              (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.conv_in.weight':                                            (13824, torch.float32, torch.Size([128, 3, 3, 3])),
    'first_stage_model.encoder.conv_out.bias':                                             (32, torch.float32, torch.Size([8])),
    'first_stage_model.encoder.conv_out.weight':                                           (147456, torch.float32, torch.Size([8, 512, 3, 3])),
    'first_stage_model.encoder.down.0.block.0.conv1.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.0.conv1.weight':                               (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.encoder.down.0.block.0.conv2.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.0.conv2.weight':                               (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.encoder.down.0.block.0.norm1.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.0.norm1.weight':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.0.norm2.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.0.norm2.weight':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.conv1.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.conv1.weight':                               (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.encoder.down.0.block.1.conv2.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.conv2.weight':                               (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.encoder.down.0.block.1.norm1.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.norm1.weight':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.norm2.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.block.1.norm2.weight':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.downsample.conv.bias':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.0.downsample.conv.weight':                             (589824, torch.float32, torch.Size([128, 128, 3, 3])),
    'first_stage_model.encoder.down.1.block.0.conv1.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.0.conv1.weight':                               (1179648, torch.float32, torch.Size([256, 128, 3, 3])),
    'first_stage_model.encoder.down.1.block.0.conv2.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.0.conv2.weight':                               (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias':                          (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight':                        (131072, torch.float32, torch.Size([256, 128, 1, 1])),
    'first_stage_model.encoder.down.1.block.0.norm1.bias':                                 (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.1.block.0.norm1.weight':                               (512, torch.float32, torch.Size([128])),
    'first_stage_model.encoder.down.1.block.0.norm2.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.0.norm2.weight':                               (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.conv1.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.conv1.weight':                               (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.encoder.down.1.block.1.conv2.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.conv2.weight':                               (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.encoder.down.1.block.1.norm1.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.norm1.weight':                               (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.norm2.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.block.1.norm2.weight':                               (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.downsample.conv.bias':                               (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.1.downsample.conv.weight':                             (2359296, torch.float32, torch.Size([256, 256, 3, 3])),
    'first_stage_model.encoder.down.2.block.0.conv1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.0.conv1.weight':                               (4718592, torch.float32, torch.Size([512, 256, 3, 3])),
    'first_stage_model.encoder.down.2.block.0.conv2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.0.conv2.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias':                          (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight':                        (524288, torch.float32, torch.Size([512, 256, 1, 1])),
    'first_stage_model.encoder.down.2.block.0.norm1.bias':                                 (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.2.block.0.norm1.weight':                               (1024, torch.float32, torch.Size([256])),
    'first_stage_model.encoder.down.2.block.0.norm2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.0.norm2.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.conv1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.conv1.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.2.block.1.conv2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.conv2.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.2.block.1.norm1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.norm1.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.norm2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.block.1.norm2.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.downsample.conv.bias':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.2.downsample.conv.weight':                             (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.3.block.0.conv1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.0.conv1.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.3.block.0.conv2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.0.conv2.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.3.block.0.norm1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.0.norm1.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.0.norm2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.0.norm2.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.conv1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.conv1.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.3.block.1.conv2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.conv2.weight':                               (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.down.3.block.1.norm1.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.norm1.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.norm2.bias':                                 (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.down.3.block.1.norm2.weight':                               (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.k.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.k.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.encoder.mid.attn_1.norm.bias':                                      (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.norm.weight':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.proj_out.bias':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.proj_out.weight':                                (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.encoder.mid.attn_1.q.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.q.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.encoder.mid.attn_1.v.bias':                                         (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.attn_1.v.weight':                                       (1048576, torch.float32, torch.Size([512, 512, 1, 1])),
    'first_stage_model.encoder.mid.block_1.conv1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_1.conv1.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.mid.block_1.conv2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_1.conv2.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.mid.block_1.norm1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_1.norm1.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_1.norm2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_1.norm2.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.conv1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.conv1.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.mid.block_2.conv2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.conv2.weight':                                  (9437184, torch.float32, torch.Size([512, 512, 3, 3])),
    'first_stage_model.encoder.mid.block_2.norm1.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.norm1.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.norm2.bias':                                    (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.mid.block_2.norm2.weight':                                  (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.norm_out.bias':                                             (2048, torch.float32, torch.Size([512])),
    'first_stage_model.encoder.norm_out.weight':                                           (2048, torch.float32, torch.Size([512])),
    'first_stage_model.post_quant_conv.bias':                                              (16, torch.float32, torch.Size([4])),
    'first_stage_model.post_quant_conv.weight':                                            (64, torch.float32, torch.Size([4, 4, 1, 1])),
    'first_stage_model.quant_conv.bias':                                                   (32, torch.float32, torch.Size([8])),
    'first_stage_model.quant_conv.weight':                                                 (256, torch.float32, torch.Size([8, 8, 1, 1])),
    'log_one_minus_alphas_cumprod':                                                        (4000, torch.float32, torch.Size([1000])),
    'model.diffusion_model.input_blocks.0.0.bias':                                         (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.0.0.weight':                                       (46080, torch.float32, torch.Size([320, 4, 3, 3])),
    'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight':                          (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.input_blocks.1.0.in_layers.0.bias':                             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.in_layers.0.weight':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.in_layers.2.bias':                             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.in_layers.2.weight':                           (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.input_blocks.1.0.out_layers.0.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.out_layers.0.weight':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.out_layers.3.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.0.out_layers.3.weight':                          (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.input_blocks.1.1.norm.bias':                                    (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.norm.weight':                                  (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.proj_in.bias':                                 (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.proj_in.weight':                               (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.input_blocks.1.1.proj_out.bias':                                (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.proj_out.weight':                              (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias':     (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight':   (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight':       (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias':     (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight':   (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight':       (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias':      (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight':    (3276800, torch.float32, torch.Size([2560, 320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias':           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight':         (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.10.0.in_layers.0.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.in_layers.0.weight':                          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.in_layers.2.weight':                          (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.10.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.10.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.11.0.in_layers.0.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.in_layers.0.weight':                          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.in_layers.2.weight':                          (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.11.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.11.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight':                          (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.input_blocks.2.0.in_layers.0.bias':                             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.in_layers.0.weight':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.in_layers.2.bias':                             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.in_layers.2.weight':                           (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.input_blocks.2.0.out_layers.0.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.out_layers.0.weight':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.out_layers.3.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.0.out_layers.3.weight':                          (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.input_blocks.2.1.norm.bias':                                    (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.norm.weight':                                  (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.proj_in.bias':                                 (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.proj_in.weight':                               (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.input_blocks.2.1.proj_out.bias':                                (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.proj_out.weight':                              (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias':     (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight':   (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight':       (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias':     (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight':   (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight':       (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight':       (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias':      (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight':    (3276800, torch.float32, torch.Size([2560, 320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias':           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight':         (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias':              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.3.0.op.bias':                                      (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.3.0.op.weight':                                    (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight':                          (3276800, torch.float32, torch.Size([640, 1280])),
    'model.diffusion_model.input_blocks.4.0.in_layers.0.bias':                             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.4.0.in_layers.0.weight':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.input_blocks.4.0.in_layers.2.bias':                             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.in_layers.2.weight':                           (7372800, torch.float32, torch.Size([640, 320, 3, 3])),
    'model.diffusion_model.input_blocks.4.0.out_layers.0.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.out_layers.0.weight':                          (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.out_layers.3.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.out_layers.3.weight':                          (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.input_blocks.4.0.skip_connection.bias':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.0.skip_connection.weight':                       (819200, torch.float32, torch.Size([640, 320, 1, 1])),
    'model.diffusion_model.input_blocks.4.1.norm.bias':                                    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.norm.weight':                                  (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.proj_in.bias':                                 (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.proj_in.weight':                               (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.input_blocks.4.1.proj_out.bias':                                (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.proj_out.weight':                              (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias':     (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight':   (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight':       (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias':     (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight':   (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight':       (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias':      (20480, torch.float32, torch.Size([5120])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight':    (13107200, torch.float32, torch.Size([5120, 640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight':         (6553600, torch.float32, torch.Size([640, 2560])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight':                          (3276800, torch.float32, torch.Size([640, 1280])),
    'model.diffusion_model.input_blocks.5.0.in_layers.0.bias':                             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.in_layers.0.weight':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.in_layers.2.bias':                             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.in_layers.2.weight':                           (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.input_blocks.5.0.out_layers.0.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.out_layers.0.weight':                          (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.out_layers.3.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.0.out_layers.3.weight':                          (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.input_blocks.5.1.norm.bias':                                    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.norm.weight':                                  (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.proj_in.bias':                                 (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.proj_in.weight':                               (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.input_blocks.5.1.proj_out.bias':                                (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.proj_out.weight':                              (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias':     (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight':   (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight':       (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias':     (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight':   (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight':       (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight':       (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias':      (20480, torch.float32, torch.Size([5120])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight':    (13107200, torch.float32, torch.Size([5120, 640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight':         (6553600, torch.float32, torch.Size([640, 2560])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias':              (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight':            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.6.0.op.bias':                                      (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.6.0.op.weight':                                    (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight':                          (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.0.in_layers.0.bias':                             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.7.0.in_layers.0.weight':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.input_blocks.7.0.in_layers.2.bias':                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.in_layers.2.weight':                           (29491200, torch.float32, torch.Size([1280, 640, 3, 3])),
    'model.diffusion_model.input_blocks.7.0.out_layers.0.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.out_layers.0.weight':                          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.out_layers.3.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.out_layers.3.weight':                          (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.7.0.skip_connection.bias':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.0.skip_connection.weight':                       (3276800, torch.float32, torch.Size([1280, 640, 1, 1])),
    'model.diffusion_model.input_blocks.7.1.norm.bias':                                    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.norm.weight':                                  (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.proj_in.bias':                                 (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.proj_in.weight':                               (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.input_blocks.7.1.proj_out.bias':                                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.proj_out.weight':                              (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias':     (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight':   (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight':       (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias':     (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight':   (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight':       (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias':      (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight':    (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight':         (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight':                          (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.0.in_layers.0.bias':                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.in_layers.0.weight':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.in_layers.2.bias':                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.in_layers.2.weight':                           (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.8.0.out_layers.0.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.out_layers.0.weight':                          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.out_layers.3.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.0.out_layers.3.weight':                          (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.input_blocks.8.1.norm.bias':                                    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.norm.weight':                                  (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.proj_in.bias':                                 (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.proj_in.weight':                               (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.input_blocks.8.1.proj_out.bias':                                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.proj_out.weight':                              (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias':     (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight':   (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight':       (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias':     (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight':   (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight':       (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight':       (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias':      (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight':    (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight':         (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight':            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.9.0.op.bias':                                      (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.input_blocks.9.0.op.weight':                                    (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.middle_block.0.emb_layers.1.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.emb_layers.1.weight':                            (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.0.in_layers.0.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.in_layers.0.weight':                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.in_layers.2.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.in_layers.2.weight':                             (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.middle_block.0.out_layers.0.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.out_layers.0.weight':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.out_layers.3.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.0.out_layers.3.weight':                            (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.middle_block.1.norm.bias':                                      (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.norm.weight':                                    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.proj_in.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.proj_in.weight':                                 (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.middle_block.1.proj_out.bias':                                  (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.proj_out.weight':                                (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight':         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias':       (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight':     (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight':         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight':         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight':         (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias':       (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight':     (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight':         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight':         (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias':        (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight':      (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight':           (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias':                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias':                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias':                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight':              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.emb_layers.1.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.emb_layers.1.weight':                            (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.middle_block.2.in_layers.0.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.in_layers.0.weight':                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.in_layers.2.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.in_layers.2.weight':                             (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.middle_block.2.out_layers.0.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.out_layers.0.weight':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.out_layers.3.bias':                              (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.middle_block.2.out_layers.3.weight':                            (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.out.0.bias':                                                    (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.out.0.weight':                                                  (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.out.2.bias':                                                    (16, torch.float32, torch.Size([4])),
    'model.diffusion_model.out.2.weight':                                                  (46080, torch.float32, torch.Size([4, 320, 3, 3])),
    'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.0.0.in_layers.0.bias':                            (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.0.0.in_layers.0.weight':                          (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.0.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.in_layers.2.weight':                          (117964800, torch.float32, torch.Size([1280, 2560, 3, 3])),
    'model.diffusion_model.output_blocks.0.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.0.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.0.0.skip_connection.weight':                      (13107200, torch.float32, torch.Size([1280, 2560, 1, 1])),
    'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.1.0.in_layers.0.bias':                            (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.1.0.in_layers.0.weight':                          (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.1.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.in_layers.2.weight':                          (117964800, torch.float32, torch.Size([1280, 2560, 3, 3])),
    'model.diffusion_model.output_blocks.1.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.1.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.1.0.skip_connection.weight':                      (13107200, torch.float32, torch.Size([1280, 2560, 1, 1])),
    'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight':                        (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.10.0.in_layers.0.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.10.0.in_layers.0.weight':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.10.0.in_layers.2.bias':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.in_layers.2.weight':                         (7372800, torch.float32, torch.Size([320, 640, 3, 3])),
    'model.diffusion_model.output_blocks.10.0.out_layers.0.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.out_layers.0.weight':                        (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.out_layers.3.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.out_layers.3.weight':                        (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.output_blocks.10.0.skip_connection.bias':                       (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.0.skip_connection.weight':                     (819200, torch.float32, torch.Size([320, 640, 1, 1])),
    'model.diffusion_model.output_blocks.10.1.norm.bias':                                  (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.norm.weight':                                (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.proj_in.bias':                               (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.proj_in.weight':                             (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.10.1.proj_out.bias':                              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.proj_out.weight':                            (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias':   (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight': (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight':     (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias':   (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight': (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight':     (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias':    (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight':  (3276800, torch.float32, torch.Size([2560, 320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias':         (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight':       (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight':                        (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.11.0.in_layers.0.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.11.0.in_layers.0.weight':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.11.0.in_layers.2.bias':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.in_layers.2.weight':                         (7372800, torch.float32, torch.Size([320, 640, 3, 3])),
    'model.diffusion_model.output_blocks.11.0.out_layers.0.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.out_layers.0.weight':                        (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.out_layers.3.bias':                          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.out_layers.3.weight':                        (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.output_blocks.11.0.skip_connection.bias':                       (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.0.skip_connection.weight':                     (819200, torch.float32, torch.Size([320, 640, 1, 1])),
    'model.diffusion_model.output_blocks.11.1.norm.bias':                                  (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.norm.weight':                                (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.proj_in.bias':                               (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.proj_in.weight':                             (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.11.1.proj_out.bias':                              (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.proj_out.weight':                            (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias':   (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight': (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight':     (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias':   (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight': (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight':     (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight':     (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias':    (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight':  (3276800, torch.float32, torch.Size([2560, 320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias':         (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight':       (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias':            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.2.0.in_layers.0.bias':                            (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.2.0.in_layers.0.weight':                          (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.2.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.in_layers.2.weight':                          (117964800, torch.float32, torch.Size([1280, 2560, 3, 3])),
    'model.diffusion_model.output_blocks.2.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.2.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.0.skip_connection.weight':                      (13107200, torch.float32, torch.Size([1280, 2560, 1, 1])),
    'model.diffusion_model.output_blocks.2.1.conv.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.2.1.conv.weight':                                 (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.0.in_layers.0.bias':                            (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.3.0.in_layers.0.weight':                          (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.3.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.in_layers.2.weight':                          (117964800, torch.float32, torch.Size([1280, 2560, 3, 3])),
    'model.diffusion_model.output_blocks.3.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.3.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.0.skip_connection.weight':                      (13107200, torch.float32, torch.Size([1280, 2560, 1, 1])),
    'model.diffusion_model.output_blocks.3.1.norm.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.norm.weight':                                 (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.proj_in.bias':                                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.proj_in.weight':                              (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.3.1.proj_out.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.proj_out.weight':                             (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias':     (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight':   (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias':          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight':        (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.0.in_layers.0.bias':                            (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.4.0.in_layers.0.weight':                          (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.4.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.in_layers.2.weight':                          (117964800, torch.float32, torch.Size([1280, 2560, 3, 3])),
    'model.diffusion_model.output_blocks.4.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.4.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.0.skip_connection.weight':                      (13107200, torch.float32, torch.Size([1280, 2560, 1, 1])),
    'model.diffusion_model.output_blocks.4.1.norm.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.norm.weight':                                 (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.proj_in.bias':                                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.proj_in.weight':                              (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.4.1.proj_out.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.proj_out.weight':                             (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias':     (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight':   (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias':          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight':        (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight':                         (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.0.in_layers.0.bias':                            (7680, torch.float32, torch.Size([1920])),
    'model.diffusion_model.output_blocks.5.0.in_layers.0.weight':                          (7680, torch.float32, torch.Size([1920])),
    'model.diffusion_model.output_blocks.5.0.in_layers.2.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.in_layers.2.weight':                          (88473600, torch.float32, torch.Size([1280, 1920, 3, 3])),
    'model.diffusion_model.output_blocks.5.0.out_layers.0.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.out_layers.0.weight':                         (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.out_layers.3.bias':                           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.out_layers.3.weight':                         (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.5.0.skip_connection.bias':                        (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.0.skip_connection.weight':                      (9830400, torch.float32, torch.Size([1280, 1920, 1, 1])),
    'model.diffusion_model.output_blocks.5.1.norm.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.norm.weight':                                 (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.proj_in.bias':                                (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.proj_in.weight':                              (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.5.1.proj_out.bias':                               (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.proj_out.weight':                             (6553600, torch.float32, torch.Size([1280, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias':    (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight':  (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight':      (6553600, torch.float32, torch.Size([1280, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight':      (3932160, torch.float32, torch.Size([1280, 768])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias':     (40960, torch.float32, torch.Size([10240])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight':   (52428800, torch.float32, torch.Size([10240, 1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias':          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight':        (26214400, torch.float32, torch.Size([1280, 5120])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias':             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight':           (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.2.conv.bias':                                   (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.5.2.conv.weight':                                 (58982400, torch.float32, torch.Size([1280, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight':                         (3276800, torch.float32, torch.Size([640, 1280])),
    'model.diffusion_model.output_blocks.6.0.in_layers.0.bias':                            (7680, torch.float32, torch.Size([1920])),
    'model.diffusion_model.output_blocks.6.0.in_layers.0.weight':                          (7680, torch.float32, torch.Size([1920])),
    'model.diffusion_model.output_blocks.6.0.in_layers.2.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.in_layers.2.weight':                          (44236800, torch.float32, torch.Size([640, 1920, 3, 3])),
    'model.diffusion_model.output_blocks.6.0.out_layers.0.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.out_layers.0.weight':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.out_layers.3.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.out_layers.3.weight':                         (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.output_blocks.6.0.skip_connection.bias':                        (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.0.skip_connection.weight':                      (4915200, torch.float32, torch.Size([640, 1920, 1, 1])),
    'model.diffusion_model.output_blocks.6.1.norm.bias':                                   (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.norm.weight':                                 (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.proj_in.bias':                                (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.proj_in.weight':                              (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.6.1.proj_out.bias':                               (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.proj_out.weight':                             (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias':     (20480, torch.float32, torch.Size([5120])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight':   (13107200, torch.float32, torch.Size([5120, 640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias':          (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight':        (6553600, torch.float32, torch.Size([640, 2560])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight':                         (3276800, torch.float32, torch.Size([640, 1280])),
    'model.diffusion_model.output_blocks.7.0.in_layers.0.bias':                            (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.7.0.in_layers.0.weight':                          (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.output_blocks.7.0.in_layers.2.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.in_layers.2.weight':                          (29491200, torch.float32, torch.Size([640, 1280, 3, 3])),
    'model.diffusion_model.output_blocks.7.0.out_layers.0.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.out_layers.0.weight':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.out_layers.3.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.out_layers.3.weight':                         (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.output_blocks.7.0.skip_connection.bias':                        (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.0.skip_connection.weight':                      (3276800, torch.float32, torch.Size([640, 1280, 1, 1])),
    'model.diffusion_model.output_blocks.7.1.norm.bias':                                   (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.norm.weight':                                 (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.proj_in.bias':                                (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.proj_in.weight':                              (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.7.1.proj_out.bias':                               (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.proj_out.weight':                             (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias':     (20480, torch.float32, torch.Size([5120])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight':   (13107200, torch.float32, torch.Size([5120, 640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias':          (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight':        (6553600, torch.float32, torch.Size([640, 2560])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight':                         (3276800, torch.float32, torch.Size([640, 1280])),
    'model.diffusion_model.output_blocks.8.0.in_layers.0.bias':                            (3840, torch.float32, torch.Size([960])),
    'model.diffusion_model.output_blocks.8.0.in_layers.0.weight':                          (3840, torch.float32, torch.Size([960])),
    'model.diffusion_model.output_blocks.8.0.in_layers.2.bias':                            (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.in_layers.2.weight':                          (22118400, torch.float32, torch.Size([640, 960, 3, 3])),
    'model.diffusion_model.output_blocks.8.0.out_layers.0.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.out_layers.0.weight':                         (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.out_layers.3.bias':                           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.out_layers.3.weight':                         (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.output_blocks.8.0.skip_connection.bias':                        (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.0.skip_connection.weight':                      (2457600, torch.float32, torch.Size([640, 960, 1, 1])),
    'model.diffusion_model.output_blocks.8.1.norm.bias':                                   (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.norm.weight':                                 (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.proj_in.bias':                                (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.proj_in.weight':                              (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.8.1.proj_out.bias':                               (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.proj_out.weight':                             (1638400, torch.float32, torch.Size([640, 640, 1, 1])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias':    (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight':  (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight':      (1638400, torch.float32, torch.Size([640, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight':      (1966080, torch.float32, torch.Size([640, 768])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias':     (20480, torch.float32, torch.Size([5120])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight':   (13107200, torch.float32, torch.Size([5120, 640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias':          (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight':        (6553600, torch.float32, torch.Size([640, 2560])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias':             (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight':           (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.2.conv.bias':                                   (2560, torch.float32, torch.Size([640])),
    'model.diffusion_model.output_blocks.8.2.conv.weight':                                 (14745600, torch.float32, torch.Size([640, 640, 3, 3])),
    'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight':                         (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.9.0.in_layers.0.bias':                            (3840, torch.float32, torch.Size([960])),
    'model.diffusion_model.output_blocks.9.0.in_layers.0.weight':                          (3840, torch.float32, torch.Size([960])),
    'model.diffusion_model.output_blocks.9.0.in_layers.2.bias':                            (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.in_layers.2.weight':                          (11059200, torch.float32, torch.Size([320, 960, 3, 3])),
    'model.diffusion_model.output_blocks.9.0.out_layers.0.bias':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.out_layers.0.weight':                         (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.out_layers.3.bias':                           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.out_layers.3.weight':                         (3686400, torch.float32, torch.Size([320, 320, 3, 3])),
    'model.diffusion_model.output_blocks.9.0.skip_connection.bias':                        (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.0.skip_connection.weight':                      (1228800, torch.float32, torch.Size([320, 960, 1, 1])),
    'model.diffusion_model.output_blocks.9.1.norm.bias':                                   (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.norm.weight':                                 (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.proj_in.bias':                                (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.proj_in.weight':                              (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.9.1.proj_out.bias':                               (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.proj_out.weight':                             (409600, torch.float32, torch.Size([320, 320, 1, 1])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight':      (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias':    (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight':  (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight':      (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight':      (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight':      (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias':    (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight':  (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight':      (409600, torch.float32, torch.Size([320, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight':      (983040, torch.float32, torch.Size([320, 768])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias':     (10240, torch.float32, torch.Size([2560])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight':   (3276800, torch.float32, torch.Size([2560, 320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias':          (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight':        (1638400, torch.float32, torch.Size([320, 1280])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias':             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight':           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias':             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight':           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias':             (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight':           (1280, torch.float32, torch.Size([320])),
    'model.diffusion_model.time_embed.0.bias':                                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.time_embed.0.weight':                                           (1638400, torch.float32, torch.Size([1280, 320])),
    'model.diffusion_model.time_embed.2.bias':                                             (5120, torch.float32, torch.Size([1280])),
    'model.diffusion_model.time_embed.2.weight':                                           (6553600, torch.float32, torch.Size([1280, 1280])),
    'model_ema.decay':                                                                     (4, torch.float32, torch.Size([])),
    'model_ema.num_updates':                                                               (4, torch.int32, torch.Size([])),
    'posterior_log_variance_clipped':                                                      (4000, torch.float32, torch.Size([1000])),
    'posterior_mean_coef1':                                                                (4000, torch.float32, torch.Size([1000])),
    'posterior_mean_coef2':                                                                (4000, torch.float32, torch.Size([1000])),
    'posterior_variance':                                                                  (4000, torch.float32, torch.Size([1000])),
    'sqrt_alphas_cumprod':                                                                 (4000, torch.float32, torch.Size([1000])),
    'sqrt_one_minus_alphas_cumprod':                                                       (4000, torch.float32, torch.Size([1000])),
    'sqrt_recip_alphas_cumprod':                                                           (4000, torch.float32, torch.Size([1000])),
    'sqrt_recipm1_alphas_cumprod':                                                         (4000, torch.float32, torch.Size([1000]))
}

for i in SD_KEYS:
    if not re.match(r"^[a-z0-9._]+$", i):
        raise ValueError("invalid relevant key", i)
    del i


def statedict_half(state_dict, print_stats: bool = True):
    halfed_cnt = 0
    halfed_bytes = 0

    for key, val in list(state_dict.items()):
        if val.dtype is torch.float32:
            halfed = val.half()
            state_dict[key] = halfed
            halfed_cnt += 1
            halfed_bytes += halfed.element_size() * halfed.nelement()

    if print_stats:
        print(f"pruned {halfed_cnt} keys, {halfed_bytes} bytes, {halfed_bytes / 1024 ** 3:.2f} GB!")

    return (halfed_cnt, halfed_bytes)


def get_strings(data: bytes):
    string_chars = set(ord(i) for i in (string.digits + string.ascii_letters + "._"))

    strings = []
    cur_start = None
    cur_end = None
    for pos, byte in enumerate(data):
        if byte in string_chars:
            if cur_start is None:
                cur_start = pos
            cur_end = pos + 1
        else:
            if cur_start is not None:
                strings.append(data[cur_start:cur_end].decode())

            cur_start = cur_end = None
    return strings


def first_num(strings):
    for pos, i in enumerate(strings):
        if not i or not i[0].isdigit():
            continue

        digits, end = re.match("^(\d+)([qr])", i).groups()
        digits = int(digits)
        if digits < 31:
            if end[0] != "q":
                raise ValueError("invalid small end", digits, end, pos, i)
        else:
            if end[0] != "r":
                raise ValueError("invalid larger end", digits, end, pos, i)

        return pos, digits

    raise ValueError("no num found", strings)


def save_load_statedict(zipfile: zipfile.ZipFile, forbid_other_keys: bool):
    data_pkl = zipfile.read("archive/data.pkl")
    sd_shortest = min(len(i) for i in SD_KEYS)

    strings = get_strings(data_pkl)
    strings = [i for i in strings if len(i) >= sd_shortest or i[0].isdigit()]

    # get data key

    pos_by_num = { }
    pos_by_sd_key = { }
    for sd_key in SD_KEYS.keys():
        for pos, i in enumerate(strings):
            if i.startswith(sd_key):
                break
        else:
            raise ValueError("couldn't find sd_key", sd_key)
        # print(f"found {sd_key!r} at {pos}")

        num_pos, num = first_num(strings[pos + 1:pos + 8])
        # could just only use pos_by_sd_key and raise on dup here
        # but nicer to have an error with all duplicate keys
        # not just the first one it failed on
        pos_by_num.setdefault(num, []).append(sd_key)
        pos_by_sd_key[sd_key] = num

    duplicate_keys = { k: v for k, v in pos_by_num.items() if len(v) > 1 }
    if duplicate_keys:
        raise ValueError("duplicate keys", duplicate_keys)

    if forbid_other_keys:
        if pos_by_num.keys() != set(range(0, len(pos_by_num))):
            raise ValueError("unknown data keys", sorted(pos_by_num.keys()))

    state_dict = { }
    for sd_key, (expected_size, dtype, shape) in SD_KEYS.items():
        num = pos_by_sd_key[sd_key]
        data_path = f"archive/data/{num}"
        data = zipfile.read(data_path)

        size_ok = len(data) == expected_size
        if not size_ok and dtype is torch.float32 and len(data) == expected_size // 2:
            print("halfed", sd_key)
            dtype = torch.float16
            size_ok = True

        if not size_ok:
            raise ValueError("invalid data size", len(data), sd_key)

        tensor = torch.frombuffer(data, dtype = dtype)
        tensor = tensor.reshape(shape)
        state_dict[sd_key] = tensor

    return state_dict


def main(input_path: str, output_path: str, overwrite: bool, half: bool, forbid_other_keys: bool):
    print(f"loading {input_path!r}")

    with zipfile.ZipFile(input_path) as zip:
        sd = save_load_statedict(zip, forbid_other_keys)

    if half:
        print("halfing")
        statedict_half(sd, True)

    model = { "state_dict": sd }

    print(f"writing to {output_path!r}, overwrite={overwrite}")
    with open(output_path, "wb" if overwrite else "xb") as out_file:
        torch.save(model, out_file)


if __name__ == "__main__":
    def setup():
        parser = argparse.ArgumentParser()
        parser.add_argument("input_file")
        parser.add_argument("output_file")
        parser.add_argument("-o", "--overwrite", action = "store_true")
        parser.add_argument("-H", "--half", action = "store_true")
        parser.add_argument("-f", "--forbid_other_keys", action = "store_true")
        args = parser.parse_args()
        main(args.input_file, args.output_file, args.overwrite, args.half, False)


    setup()
